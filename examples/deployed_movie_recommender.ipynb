{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-CivisML Model Deployment\n",
    "\n",
    "Thanks to the flexible nature of Civis's `/services` endpoint, we can deploy much more than just CivisML models. In fact, more or less any app that lives in a Docker container and serves requests can be deployed via Civis Platform. \n",
    "\n",
    "As an example of a non-CivisML deployed model, here we will train a neural network to create movie recommendations using the free \"MovieLens\" movie rating data. This model is just for demonstration purposes and not at well tuned, but hopefully it will provide inspiration for users to write their own apps and deploy them on Civis Platform. The code that generated the Docker image that we use below (\"civisanalytics/docker-scratch:muffnn_recommender-v0.0.6\") can be found [here](https://github.com/civisanalytics/model-deployment/tree/muffnn_recommendation_engine). For now, a pre-built image is only accessible through Civis's private Dockerhub account, but users without access to Civis's Dockerhub can build the image themselves from the above-linked repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from io import BytesIO, StringIO\n",
    "import pickle\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import civis  # Must be version 1.8.X\n",
    "from muffnn import MLPRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build a movie recommendation engine\n",
    "\n",
    "We'll use the small MovieLens set to train an MLP. Let's get the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://files.grouplens.org/datasets/movielens/ml-latest-small.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "\n",
    "with ZipFile(BytesIO(response.content)) as zfile:\n",
    "    ratings = pd.read_csv(BytesIO(zfile.read('ml-latest-small/ratings.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(ratings.shape)\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape the data into a wide, sparse format for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell requires a decent amount of memory. It also takes about a minute to run.\n",
    "ymlp = ratings['rating']\n",
    "xmlp = pd.get_dummies(ratings.drop(['rating', 'timestamp'], axis=1), columns=['userId', 'movieId'])\n",
    "\n",
    "xtrain = sparse.csr_matrix(xmlp.values)\n",
    "ytrain = ymlp.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xtrain.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "We'll make a simple MLP regressor to predict movie ratings. This model has not been meaningfully tuned, but it will do for the sake of providing an example recommendation engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp = MLPRegressor(\n",
    "    hidden_units=[200],\n",
    "    n_epochs=20,\n",
    "    keep_prob=0.6,\n",
    "    batch_size=200,\n",
    "    random_state=np.random.RandomState(3)\n",
    ")\n",
    "# Training takes about a minute\n",
    "mlp.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model and user-item columns.\n",
    "\n",
    "In order to deploy our model, we'll save it to the Civis /files endpoint. \n",
    "\n",
    "Additionally, we'll save the list of columns in the wide training data. This will enable us to reconstruct the shape of the sparse input data expected from our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with BytesIO() as bts:\n",
    "    pickle.dump(mlp, bts)\n",
    "\n",
    "    bts.seek(0)\n",
    "    model_file_id = civis.io.file_to_civis(bts, 'movie_mlp.pkl', expires_at=None)\n",
    "\n",
    "    print(\"Model file ID: \", model_file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with StringIO() as sio:\n",
    "    # Write comma-separated column list to StringIO\n",
    "    sio.write(','.join(xmlp.columns))\n",
    "\n",
    "    sio.seek(0)\n",
    "    user_item_columns_file_id = civis.io.file_to_civis(sio, 'columns.txt', expires_at=None)\n",
    "\n",
    "    print(\"Column list file ID: \", user_item_columns_file_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model\n",
    "\n",
    "To deploy a recommendation engine, we'll make the same sequence of API calls we made to deploy a standard CivisML model. However due to the data transformations necessary to build the sparse input matrix and create the ranked recommendations, we need to pass a few more environment variables. These contain the metadata required to generate the sparse input matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = civis.APIClient(resources='all')\n",
    "\n",
    "resp = client.services.post(\n",
    "        name='deployed_recommender', \n",
    "        docker_image_name=\"civisanalytics/docker-scratch\", \n",
    "        docker_image_tag=\"muffnn_recommender-v0.0.6\", \n",
    "        cpu=1000,\n",
    "        memory=8000, \n",
    "        environment_variables={'DEBUG': 1, 'MODEL_FILE_ID': model_file_id, \n",
    "                               'USER_ITEM_COLUMNS_FILE_ID': user_item_columns_file_id, \n",
    "                               'USER_PREFIX': 'userId_', 'ITEM_PREFIX': 'movieId_'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have created our service, we can start the deployment with the following API call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = client.services.post_deployments(resp['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get some recommendations!\n",
    "\n",
    "As usual, we'll want to grab our URL and create an access token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = client.services.get(resp['id'])['current_url']\n",
    "print(\"URL: \", url)\n",
    "\n",
    "token_resp = client.services.post_tokens(resp['id'], 'my_token')\n",
    "print(\"Token: \", token_resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our deployed recommender will take a user ID as its input, and output one recommendation by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Put the token in the header of the HTTP call\n",
    "headers = {\"Authorization\": \"Bearer {}\".format(token_resp['token'])}\n",
    "\n",
    "pred_url = url + '/predict?user=10'\n",
    "\n",
    "# Make the GET call\n",
    "getresp = requests.get(pred_url, headers=headers)\n",
    "print(getresp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, you can use the `ntopitems` parameter to specify the number of items you would like recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_url = url + '/predict?user=10&ntopitems=5'\n",
    "\n",
    "# Make the GET call\n",
    "getresp = requests.get(pred_url, headers=headers)\n",
    "print(getresp.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
